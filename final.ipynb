{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa067e4-862f-4937-94a5-6effc1cd8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 50  \n",
    "pd.options.display.max_rows = 500     \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6fa64-98c4-4233-8c14-a868209f61c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_embed = pd.read_csv('embeddings.csv')\n",
    "df_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735260bc-ebbd-41ae-9603-bc8e6465e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = df_embed.iloc[:, 1:].mean()  \n",
    "mean_vals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015a674-9683-4bf8-8640-96a1a655c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_csv('metadata.csv')\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8fe49-cb5a-4d0b-87d1-f6d6ef3090dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_embed, df_meta, on='site_id')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714c768-d6ef-4648-a17e-d19b53310e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cell_types = merged_df['cell_type'].nunique()\n",
    "unique_sirnas = merged_df['sirna'].nunique()\n",
    "unique_well_types = merged_df['well_type'].nunique()\n",
    "\n",
    "feature_summary = merged_df.describe().transpose()\n",
    "\n",
    "missing_values = merged_df.isnull().sum()\n",
    "\n",
    "(unique_cell_types, unique_sirnas, unique_well_types, feature_summary.head(), missing_values.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f816683-5458-42d0-9fe8-76bb01a6beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f93fd-ea10-4b83-bfd2-6e4ff6250e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "import os.path\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d2fd5-f9e4-4975-a899-4fe9f8972f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "feature_cols = [col for col in merged_df.columns if col.startswith('feature_')]\n",
    "X_scaled = scaler.fit_transform(merged_df[feature_cols])\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X_scaled)\n",
    "principalDf = pd.DataFrame(data=principalComponents, columns=['principal component 1', 'principal component 2'])\n",
    "\n",
    "# KMeans clustering on the PCA results\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(principalDf)\n",
    "principalDf['cluster'] = kmeans.labels_\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x='principal component 1', y='principal component 2',\n",
    "    hue='cluster', data=principalDf, palette=sns.color_palette(\"hsv\", 5),\n",
    "    legend=\"full\", alpha=0.3\n",
    ")\n",
    "plt.title('PCA Clustering Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0009b-c237-4dc5-aea4-12bc15eb09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a9bfc-3753-4113-8078-7853f4512f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_data = merged_df[feature_cols + ['sirna']]\n",
    "train_df, test_df = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "train_df['sirna_encoded'] = encoder.fit_transform(train_df['sirna'])\n",
    "test_df['sirna_encoded'] = encoder.transform(test_df['sirna'])\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['sirna_encoded']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['sirna_encoded']  \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172bf14-2251-4d76-9923-8fbc2619b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_data = merged_df[feature_cols + ['sirna']]\n",
    "train_df, test_df = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "train_df['sirna_encoded'] = encoder.fit_transform(train_df['sirna'])\n",
    "test_df['sirna_encoded'] = encoder.transform(test_df['sirna'])\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['sirna_encoded']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['sirna_encoded']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2354437-6b02-4c70-abd9-4eaa6b978cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splits = 4\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=2997)\n",
    "\n",
    "xkf_train=[]\n",
    "xkf_val=[]\n",
    "ykf_train=[]\n",
    "ykf_val=[]\n",
    "x_train_arr = X_train.values\n",
    "y_train_arr = y_train.values\n",
    "for train_index, test_index in kf.split(x_train_arr):\n",
    "    Xtrainfold, Xvalfold = x_train_arr[train_index], x_train_arr[test_index]\n",
    "    Ytrainfold, Yvalfold = y_train_arr[train_index], y_train_arr[test_index]\n",
    "    Ytrainfold = np.squeeze(Ytrainfold)\n",
    "    Yvalfold = np.squeeze(Yvalfold)\n",
    "    xkf_train.append(Xtrainfold)\n",
    "    xkf_val.append(Xvalfold)\n",
    "    ykf_train.append(Ytrainfold)\n",
    "    ykf_val.append(Yvalfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766b7ae-aad4-4317-8201-18ea699ddc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def printScores(y_val, class_pred, proba, average='macro'):\n",
    "    f1 = f1_score(y_val, class_pred, average=average)\n",
    "    precision = precision_score(y_val, class_pred, average=average)\n",
    "    recall = recall_score(y_val, class_pred, average=average)\n",
    "    accuracy = accuracy_score(y_val, class_pred)\n",
    "    # For ROC AUC, you might want to handle multiclass separately\n",
    "    # auc = roc_auc_score(y_val, proba, multi_class='ovr')  # Example for OvR approach\n",
    "    print(confusion_matrix(y_val, class_pred))\n",
    "    print(\"F1: \" + str(f1))\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "\n",
    "def returnscores_xv(Yval, predictions, average='macro'):\n",
    "    score = np.zeros(4)\n",
    "    score[0] = accuracy_score(Yval, predictions)\n",
    "    score[1] = recall_score(Yval, predictions, average=average, zero_division=0)\n",
    "    score[2] = precision_score(Yval, predictions, average=average, zero_division=0)\n",
    "    score[3] = f1_score(Yval, predictions, average=average)\n",
    "    return score\n",
    "\n",
    "def printscores_xv(scores):\n",
    "    print(\"Accuracy: \" + str(100 * scores[0]) + \"%\")\n",
    "    print(\"Recall: \" + str(100 * scores[1]) + \"%\")\n",
    "    print(\"Precision: \" + str(100 * scores[2]) + \"%\")\n",
    "    print(\"F1: \" + str(100 * scores[3]) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f53470-7b17-4757-87c7-de234f0cfaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert DataFrame to numpy arrays for cross-validation\n",
    "x_train_arr = X_train.values\n",
    "y_train_arr = y_train.values\n",
    "x_test_arr = X_test.values\n",
    "y_test_arr = y_test.values\n",
    "\n",
    "# Setting up K-Fold cross-validation\n",
    "num_splits = 4\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=2997)\n",
    "\n",
    "# Preparing lists to store train and validation data for each fold\n",
    "xkf_train = []\n",
    "xkf_val = []\n",
    "ykf_train = []\n",
    "ykf_val = []\n",
    "\n",
    "# Splitting the data for each fold\n",
    "for train_index, test_index in kf.split(x_train_arr):\n",
    "    Xtrainfold, Xvalfold = x_train_arr[train_index], x_train_arr[test_index]\n",
    "    Ytrainfold, Yvalfold = y_train_arr[train_index], y_train_arr[test_index]\n",
    "    xkf_train.append(Xtrainfold)\n",
    "    xkf_val.append(Xvalfold)\n",
    "    ykf_train.append(Ytrainfold)\n",
    "    ykf_val.append(Yvalfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8bfffc-48ac-4d56-8776-6f9dded0481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "scores_lr = np.zeros((num_splits, 4))  # Assuming 4 scores: accuracy, recall, precision, F1\n",
    "for split, (train_index, test_index) in tqdm(enumerate(kf.split(X_train)), total=num_splits, desc='Logistic Regression Progress'):\n",
    "    Xtrainfold, Xvalfold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    Ytrainfold, Yvalfold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    # Scaling within the loop\n",
    "    scaler = StandardScaler()\n",
    "    Xtrainfold_scaled = scaler.fit_transform(Xtrainfold)\n",
    "    Xvalfold_scaled = scaler.transform(Xvalfold)\n",
    "\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "    log_reg.fit(Xtrainfold_scaled, Ytrainfold)\n",
    "    predictions = log_reg.predict(Xvalfold_scaled)\n",
    "    scores_lr[split, :] = returnscores_xv(Yvalfold, predictions)\n",
    "\n",
    "average_scores_lr = np.mean(scores_lr, axis=0)\n",
    "print(\"Average Scores for Logistic Regression:\")\n",
    "printscores_xv(average_scores_lr)\n",
    "\n",
    "# Test Set Evaluation\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_predictions = log_reg.predict(X_test_scaled)\n",
    "print(\"Test Set Scores for Logistic Regression:\")\n",
    "printScores(y_test, test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cbc4e-f7c9-4c02-9cb8-009605b72497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a2636-7f21-4dbd-b77a-79fa5c240bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "scores_lr = np.zeros((num_splits, n_scores))\n",
    "for split in tqdm(range(num_splits), desc='Logistic Regression Progress'):\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "    log_reg.fit(xkf_train[split], ykf_train[split])\n",
    "    scores_lr[split, :] = returnscores_xv(ykf_val[split], log_reg.predict(xkf_val[split]))\n",
    "average_scores_lr = np.mean(scores_lr, axis=0)\n",
    "print(\"Average Scores for Logistic Regression:\")\n",
    "printscores_xv(average_scores_lr)\n",
    "print(\"Test Set Scores for Logistic Regression:\")\n",
    "DNN.fit(x_train_arr, y_train_arr)\n",
    "proba = DNN.predict_proba(x_test_arr)[:, 1]\n",
    "printScores(y_test_arr, DNN.predict(x_test_arr), proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb75dee-4d92-4180-8cb5-f7683521cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna\n",
    "!pip install pyDOE2\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0d354-677a-4fd1-98b6-d27d93acbbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import time\n",
    "import pyDOE2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f23135-b3ac-47bb-b754-bf98bfdd60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outermost Function Call for hyperparameter tuning.\n",
    "Takes in dataframe of optimization parameters, datasets, parameter names, algorithm name\n",
    "Takes in reduction parameter for Generalized Subspace Design for grid search (reduction)\n",
    "Takes in the number of retrainings for each hyperparameter config (n_trials), number of Cross Val splits (n_splits),\n",
    "number of iterations to run BO (n_bo) number of instantiations of the final hyperparameter configuration to test\n",
    "First, we set up a wrapper function for the call to fit_bo that we can use with Optuna.\n",
    "Nest, we set up the pruner and the study, then perform the initial grid sampling.\n",
    "We then reset the sampler for our study to the TPESampler and perform BO to find the best hyperparameter configuration\n",
    "Finally, we call preparDF, saving a report, and search for an optimal instantiation of a model to return.\n",
    "'''\n",
    "\n",
    "def hyperparam_search(df, function, x_train, x_test, y_train, y_test, indexnames, name, reduction=1, n_trials = 10, n_splits = 5, n_bo=100, n_inst=100, sklearn=True):\n",
    "    print(np.shape(x_test))\n",
    "    def wrapper(trial): #Wrapper function for fit_bo\n",
    "        return fit_bo(trial, function, df, x_train, y_train, n_trials, n_splits)\n",
    "    start_time = time.time()\n",
    "    pruner = optuna.pruners.PercentilePruner(75.0, n_startup_trials=1, n_warmup_steps=3, interval_steps=1) #Pruner\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner)  # Create a new study using pruner.\n",
    "    n_configs = init_gridsample(study, wrapper, df, reduction=reduction) #Perform initial grid sampling\n",
    "    study.sampler = optuna.samplers.TPESampler() #Set sampler to Tree-Structured Parzen Estimator\n",
    "    study.optimize(wrapper, n_trials=n_bo) #Optimize\n",
    "    bestparams = study.best_params\n",
    "    prepareDF(df, bestparams, indexnames, name)\n",
    "    model = test_instantiations(function, bestparams, x_train, y_train, n_inst, sklearn)\n",
    "    bestf1, auc, precision, recall, accuracy = score(model, x_test, y_test, sklearn)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Perform the initialization of the BO using grid sampling.\n",
    "First call get_configlist to get the list of grid sampling configurations\n",
    "Then for each configuration, initialize a single point grid sample search in optuna\n",
    "Run 1 trial for each grid point\n",
    "'''\n",
    "def init_gridsample(study, func, df, reduction=5):\n",
    "    configs=get_configlist(df, reduction) # Get the DOE configurations\n",
    "    for config in configs: #Loop over all DOE configs\n",
    "\n",
    "        #Create the search space\n",
    "        search_space={}\n",
    "        for i in range(len(df.index)):\n",
    "            search_space[df.index[i]]=[config[i]]\n",
    "\n",
    "        #Set the study's sampler to be single point gridsearch space, then do one round of fitting\n",
    "        study.sampler=optuna.samplers.GridSampler(search_space)\n",
    "        study.optimize(func, n_trials=1)\n",
    "    return len(configs)\n",
    "\n",
    "'''\n",
    "Calculate gridsearch configurations. If we have a reduction greater than one, use GSD to reduce\n",
    "For each configuration, calculate the corresponding parameter values using get_gridvalm then return\n",
    "'''\n",
    "def get_configlist(df, reduction=5):\n",
    "    #Create a list of the number of grid locations for parameter.\n",
    "    #Cont & int get 2 locations, Cat gets locations equal to # of discrete categories\n",
    "    configsize=[]\n",
    "    for parameter in df.index:\n",
    "        if df.loc[parameter, \"Datatype\"]==\"Categorical\":\n",
    "            configsize.append(len(df.loc[parameter, \"Values/Min-Max\"]))\n",
    "        else:\n",
    "            configsize.append(df.loc[parameter, \"Gridres\"])\n",
    "\n",
    "    print(configsize)\n",
    "\n",
    "    #If reduction>1 we call gsd, otherwise do full fact\n",
    "    #Uses pyDOE2's Generalized subset designs\n",
    "    if reduction>1:\n",
    "        DOEconfigs=pyDOE2.gsd(configsize, reduction=reduction)\n",
    "    else:\n",
    "        DOEconfigs=pyDOE2.fullfact(configsize).astype(int)\n",
    "    print(\"Number of GridSearch configs: \" + str(len(DOEconfigs)))\n",
    "\n",
    "    #Look up actual gridsearch values from indices\n",
    "    configvals=[]\n",
    "    for i in range(len(DOEconfigs)):\n",
    "        newvals=[]\n",
    "        for j in range(len(DOEconfigs[0])):\n",
    "            newvals.append(get_gridval(df, df.index[j], DOEconfigs[i][j]))\n",
    "        configvals.append(newvals)\n",
    "    return configvals\n",
    "\n",
    "\n",
    "'''\n",
    "Calculate the value of a particular grid sampling point\n",
    "Categorical, continuous, and discrete variables are handled individually\n",
    "Log scale our calculations when the variable is log scaled\n",
    "We assume grid points are evenly spaced between limits\n",
    "'''\n",
    "def get_gridval(df, parameter, index):\n",
    "    #If categorical, we simply return the value corresponidng to the index\n",
    "    if df.loc[parameter, \"Datatype\"]==\"Categorical\":\n",
    "        return df.loc[parameter, \"Values/Min-Max\"][index]\n",
    "\n",
    "    #Grab vals for minval, maxval and scaling from df for convenience\n",
    "    minval=df.loc[parameter, \"Values/Min-Max\"][0]\n",
    "    maxval=df.loc[parameter, \"Values/Min-Max\"][1]\n",
    "    scaling=df.loc[parameter, \"Logscaling\"]\n",
    "    gridres=df.loc[parameter, \"Gridres\"]\n",
    "\n",
    "    #Calculate the percentile between the parameter limits to sample the grid point\n",
    "    gridloc=index/gridres+1/gridres/2\n",
    "\n",
    "    #scaling is true, we will logscale when performing our calculations\n",
    "    if scaling==True:\n",
    "        #Indices 0 and 1 should be at 25th and 75th percentile of parameter ranges, respecively\n",
    "        value=np.exp((1-gridloc)*np.log(minval)+gridloc*np.log(maxval))\n",
    "    else:\n",
    "        value=(1-gridloc)*minval+gridloc*maxval\n",
    "\n",
    "    #If we have an integer parameter, we round to the nearest integer\n",
    "    if df.loc[parameter, \"Datatype\"]==\"Continuous\":\n",
    "        return value\n",
    "    if df.loc[parameter, \"Datatype\"]==\"Integer\":\n",
    "        return round(value)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Sample a particular parameter from the optuna trial.\n",
    "Automates the sampling call based on the information about the parameter contained in DF\n",
    "\n",
    "This function duals as a simple dictionary lookup if trial is a dictionary.\n",
    "This allows the reuse of fit_XXX functions when selecting an instantiation for a given config\n",
    "'''\n",
    "def sample(trial, df, parameter):\n",
    "    #If we have passed in a dictionary, simply index a value from the dictionary.\n",
    "    if type(trial)==dict:\n",
    "        return trial.get(parameter)\n",
    "\n",
    "    #We setup each call to the trial in the format Optuna expects. See the Optuna docs\n",
    "    if df.loc[parameter, \"Datatype\"]==\"Categorical\":\n",
    "        return trial.suggest_categorical(parameter, df.loc[parameter, \"Values/Min-Max\"])\n",
    "    #Grab vals for minval, maxval and scaling from df for convenience\n",
    "    minval=df.loc[parameter, \"Values/Min-Max\"][0]\n",
    "    maxval=df.loc[parameter, \"Values/Min-Max\"][1]\n",
    "    scaling=df.loc[parameter, \"Logscaling\"]\n",
    "    if df.loc[parameter, \"Datatype\"]==\"Continuous\":\n",
    "        return trial.suggest_float(parameter, minval, maxval, log=scaling)\n",
    "    if df.loc[parameter, \"Datatype\"]==\"Integer\":\n",
    "        return trial.suggest_int(parameter, minval, maxval, 1, log=scaling)\n",
    "\n",
    "\n",
    "'''\n",
    "General BO fit loop. For each trial in BO we will create n_trials k_fold splits where k is n_splits\n",
    "This yields a total of n_trial*n_folds fitting runs.\n",
    "Since we apply the hyperparameters to each model differently, we call the func function, which is specified\n",
    "This func function will be a unique function for each type of model which will assign the hyperparameters\n",
    "The func function will then fit the model and return the model back. For each run, we score on the val set.\n",
    "We pass intermediate scores in a report to Optuna so it can determine if the trial should be pruned\n",
    "If the trial is pruned, the trial prematurely exits\n",
    "'''\n",
    "def fit_bo(trial, func, df, xdata, ydata, n_trials, n_splits):\n",
    "    valf1 = 0\n",
    "    stepcount = 0\n",
    "    with tqdm(total=n_splits * n_trials) as pbar:\n",
    "        for j in range(n_trials):\n",
    "            kf = KFold(n_splits=n_splits, random_state=None, shuffle=False)\n",
    "            for train_index, test_index in kf.split(xdata):\n",
    "                x_train, x_val = xdata[train_index], xdata[test_index]\n",
    "                y_train, y_val = ydata[train_index], ydata[test_index]\n",
    "\n",
    "                model = func(trial, df, x_train, x_val, y_train, y_val)\n",
    "\n",
    "                # Adjust metric calculation for multiclass\n",
    "                instancef1 = sklearn.metrics.f1_score(y_val, model.predict(x_val), average='macro')\n",
    "                valf1 += instancef1\n",
    "                trial.report(valf1 / (stepcount + 1), stepcount)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "                pbar.update(1)\n",
    "                stepcount += 1\n",
    "        return valf1 / n_splits / n_trials\n",
    "\n",
    "\n",
    "\n",
    "def plot_res(study): #Some Optuna plots\n",
    "    optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "    optuna.visualization.matplotlib.plot_intermediate_values(study)\n",
    "    optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "    optuna.visualization.matplotlib.plot_slice(study)\n",
    "\n",
    "\n",
    "'''\n",
    "Select an optimal instantiation of a model.\n",
    "We have defined a constant random seed to ensure train and val sets across all models are consistent\n",
    "numtest models are tested, all instantiated using bestparams.\n",
    "'''\n",
    "def test_instantiations(func, bestparams, xdata, ydata, numtest=100, sklearn_like=True):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(xdata, ydata, test_size=0.2, random_state=1)\n",
    "    bestf1=float(\"-inf\")\n",
    "    for i in tqdm(range(numtest)):\n",
    "        #True flag indicates to we are testing instantiations. Used to predict probability in models like SVM\n",
    "        #We need probabilities for AUC, but they cause slower fitting\n",
    "        model = func(bestparams, None, x_train, x_val, y_train, y_val, True)\n",
    "        valf1 = sklearn.metrics.f1_score(y_val, np.rint(model.predict(x_val)))\n",
    "        if valf1>bestf1:\n",
    "            bestf1 = valf1\n",
    "            bestmodel = model\n",
    "    class_pred=bestmodel.predict(x_val)\n",
    "    return bestmodel\n",
    "\n",
    "'''\n",
    "Score the model in the test set\n",
    "If sklearn-like is true, the class probabilities are given by predict_proba and predict returns classes\n",
    "Otherwise, predict returns probabilities and we need to round to get the classes.\n",
    "We handle these cases separately. We report F1, AUC, Precision, Recall, and Accuracy.\n",
    "'''\n",
    "def score(model, xdata, ydata, sklearn_like=True):\n",
    "    class_pred=model.predict(xdata)\n",
    "    if sklearn_like==True: #If class_pred has class values, we need to use predict_proba for probability values\n",
    "        auc = roc_auc_score(ydata, model.predict_proba(xdata)[:, 1])\n",
    "    else: #If class_pred currently has probability values\n",
    "        auc = roc_auc_score(ydata, class_pred)\n",
    "        class_pred = np.rint(class_pred)\n",
    "    precision = precision_score(ydata, class_pred)\n",
    "    recall = recall_score(ydata, class_pred)\n",
    "    accuracy = accuracy_score(ydata, class_pred)\n",
    "    f1 = f1_score(ydata, class_pred)\n",
    "    print(confusion_matrix(ydata, class_pred, labels=[0, 1]))\n",
    "    print(\"F1: \" + str(f1))\n",
    "    print(\"AUC: \" + str(auc))\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    return f1, auc, precision, recall, accuracy\n",
    "\n",
    "'''\n",
    "Print out a clean(ish) report of hyperparameter configurations\n",
    "'''\n",
    "def prepareDF(df, bestparams, indexnames, name):\n",
    "    df = df.copy()\n",
    "    df.columns=[\"Datatype\", \"Values/[Min, Max]\", \"Log Scaling\", \"Gridpoint Count\"]\n",
    "    df[\"Best Value\"] = bestparams.values()\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    for index in df.index:\n",
    "        if df.at[index, \"Datatype\"]==\"Integer\":\n",
    "            df.at[index, \"Best Value\"]=int(df.at[index, \"Best Value\"])\n",
    "    df.index=indexnames\n",
    "    df.columns.name = \"Hyperparameter\"\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8b485-15c5-4ae7-b719-1e4acfc3914a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00304d-89e1-4f11-96df-20603000f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Bayesian hyperparameter optimization logistic regression\n",
    "\n",
    "def fit_logistic_regression(trial_or_dict, df, X_train, X_val, y_train, y_val, inst=False):\n",
    "    # Sample parameters\n",
    "    C = sample(trial_or_dict, df, \"C\")\n",
    "    solver = sample(trial_or_dict, df, \"solver\")\n",
    "    penalty = sample(trial_or_dict, df, \"penalty\")\n",
    "    max_iter = sample(trial_or_dict, df, \"max_iter\")\n",
    "    tol = sample(trial_or_dict, df, \"tol\")\n",
    "    class_weight = sample(trial_or_dict, df, \"class_weight\")\n",
    "    l1_ratio = None\n",
    "    if penalty == \"elasticnet\":\n",
    "        l1_ratio = sample(trial_or_dict, df, \"l1_ratio\")\n",
    "\n",
    "    # Instantiate and fit the Logistic Regression model\n",
    "    model = LogisticRegression(C=C, solver=solver, penalty=penalty, max_iter=max_iter, \n",
    "                               tol=tol, class_weight=class_weight, l1_ratio=l1_ratio, \n",
    "                               multi_class='ovr')  # or 'multinomial'\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "df = pd.DataFrame()\n",
    "df[\"C\"] = [\"Continuous\", [0.001, 10], True, 5]\n",
    "df[\"solver\"] = [\"Categorical\", [\"liblinear\", \"newton-cg\", \"lbfgs\", \"sag\", \"saga\"], False, \"N/A\"]\n",
    "df[\"penalty\"] = [\"Categorical\", [\"l1\", \"l2\", \"elasticnet\", \"none\"], False, \"N/A\"]\n",
    "df[\"max_iter\"] = [\"Integer\", [100, 1000], False, 5]\n",
    "df[\"tol\"] = [\"Continuous\", [1e-4, 1e-2], True, 5]\n",
    "df[\"class_weight\"] = [\"Categorical\", [\"balanced\", None], False, \"N/A\"]\n",
    "df[\"l1_ratio\"] = [\"Continuous\", [0, 1], False, 5]  # Only relevant if penalty is 'elasticnet'\n",
    "\n",
    "df = df.transpose()\n",
    "df.columns = [\"Datatype\", \"Values/Min-Max\", \"Logscaling\", \"Gridres\"]\n",
    "lrdf = df\n",
    "\n",
    "indexnames = [\"Inverse of Regularization Strength\", \"Optimization Algorithm\", \"Penalty Norm\", \n",
    "              \"Maximum Iterations\", \"Tolerance for Stopping\", \"Class Weight\", \"Elastic-Net Mixing Parameter\"]\n",
    "model = hyperparam_search(lrdf, fit_logistic_regression, X_train.values, X_test.values, \n",
    "                          y_train.values, y_test.values, indexnames, \"LogisticRegression\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984781d-d20c-4a17-bd58-616102608a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157620e-956e-4fad-822c-9e1b16db9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_data = merged_df[feature_cols + ['sirna']]\n",
    "train_df, test_df = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "train_df['sirna_encoded'] = encoder.fit_transform(train_df['sirna'])\n",
    "test_df['sirna_encoded'] = encoder.transform(test_df['sirna'])\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['sirna_encoded']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['sirna_encoded']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5543b-5a2f-4384-8468-e4e87faa57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbab74-3235-4a3d-b52d-cc0f52f669a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1d2e7-1f33-4dec-92c2-af47924ccf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28781b-cd18-41c0-aac6-90d7acf28b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac760a-9e5d-45dd-a74a-ed87af31ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from autogluon.common.utils.utils import setup_outputdir\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "from autogluon.core.utils.savers import save_pkl\n",
    "import os.path\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feature_cols = [col for col in merged_df.columns if col.startswith('feature_')]\n",
    "X_scaled = scaler.fit_transform(merged_df[feature_cols])\n",
    "\n",
    "\n",
    "all_data = merged_df[feature_cols + ['sirna']]\n",
    "train_df, test_df = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "train_df['sirna_encoded'] = encoder.fit_transform(train_df['sirna'])\n",
    "test_df['sirna_encoded'] = encoder.transform(test_df['sirna'])\n",
    "\n",
    "train_df = train_df.drop(columns=['sirna'])\n",
    "test_df = test_df.drop(columns=['sirna'])\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['sirna_encoded']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['sirna_encoded']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268f646-7048-4445-84fa-ba1ebd0f8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Existing hyperparameters\n",
    "hyperparameters = {\n",
    "    'NN_TORCH': {},\n",
    "    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
    "    'CAT': {},\n",
    "    'XGB': {},\n",
    "    'FASTAI': {},\n",
    "    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini'}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr'}}],\n",
    "    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini'}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr'}}],\n",
    "    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
    "    # Add Logistic Regression\n",
    "    'LR': {}  # Using default hyperparameters for Logistic Regression\n",
    "}\n",
    "\n",
    "# Fit the predictor\n",
    "predictor = TabularPredictor(label='sirna_encoded', problem_type='multiclass').fit(\n",
    "    train_data=train_df,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827972ff-9721-45a2-b9b7-8a0ce6813478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def printScores(y_val, class_pred):\n",
    "    f1 = f1_score(y_val, class_pred, average='macro')\n",
    "    precision = precision_score(y_val, class_pred, average='macro')\n",
    "    recall = recall_score(y_val, class_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_val, class_pred)\n",
    "    print(confusion_matrix(y_val, class_pred))\n",
    "    print(\"F1: \" + str(f1))\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    \n",
    "def returnscores_xv(Yval, predictions):\n",
    "    score = np.zeros(4)\n",
    "    score[0] = accuracy_score(Yval, predictions)\n",
    "    score[1] = recall_score(Yval, predictions, average='macro', zero_division=0)\n",
    "    score[2] = precision_score(Yval, predictions, average='macro', zero_division=0)\n",
    "    score[3] = f1_score(Yval, predictions, average='macro')\n",
    "    return score\n",
    "\n",
    "def printscores_xv(scores):\n",
    "    print(\"Accuracy: \" + str(100 * scores[0]) + \"%\")\n",
    "    print(\"Recall: \" + str(100 * scores[1]) + \"%\")\n",
    "    print(\"Precision: \" + str(100 * scores[2]) + \"%\")\n",
    "    print(\"F1: \" + str(100 * scores[3]) + \"%\")\n",
    "\n",
    "predictions = predictor.predict(X_test)\n",
    "\n",
    "proba = predictor.predict_proba(X_test)\n",
    "\n",
    "printScores(y_test, predictions)\n",
    "\n",
    "scores = returnscores_xv(y_test, predictions)\n",
    "printscores_xv(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd92704-4e16-449e-9f65-623ef5830597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, log_loss\n",
    "import time\n",
    "\n",
    "def printScores(y_val, class_pred, proba):\n",
    "    f1 = f1_score(y_val, class_pred, average='macro')\n",
    "    precision = precision_score(y_val, class_pred, average='macro')\n",
    "    recall = recall_score(y_val, class_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_val, class_pred)\n",
    "    auc = roc_auc_score(y_val, proba, multi_class='ovr')\n",
    "    logloss = log_loss(y_val, proba)\n",
    "    print(confusion_matrix(y_val, class_pred))\n",
    "    print(\"F1: \" + str(f1))\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    print(\"AUC-ROC: \" + str(auc))\n",
    "    print(\"Log Loss: \" + str(logloss))\n",
    "    \n",
    "def returnscores_xv(Yval, predictions, proba):\n",
    "    score = np.zeros(6)\n",
    "    score[0] = accuracy_score(Yval, predictions)\n",
    "    score[1] = recall_score(Yval, predictions, average='macro', zero_division=0)\n",
    "    score[2] = precision_score(Yval, predictions, average='macro', zero_division=0)\n",
    "    score[3] = f1_score(Yval, predictions, average='macro')\n",
    "    score[4] = roc_auc_score(Yval, proba, multi_class='ovr')\n",
    "    score[5] = log_loss(Yval, proba)\n",
    "    return score\n",
    "\n",
    "def printscores_xv(scores):\n",
    "    print(\"Accuracy: \" + str(100 * scores[0]) + \"%\")\n",
    "    print(\"Recall: \" + str(100 * scores[1]) + \"%\")\n",
    "    print(\"Precision: \" + str(100 * scores[2]) + \"%\")\n",
    "    print(\"F1: \" + str(100 * scores[3]) + \"%\")\n",
    "    print(\"AUC-ROC: \" + str(100 * scores[4]) + \"%\")\n",
    "    print(\"Log Loss: \" + str(scores[5]))\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "predictions = predictor.predict(X_test)\n",
    "proba = predictor.predict_proba(X_test)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "printScores(y_test, predictions, proba)\n",
    "\n",
    "scores = returnscores_xv(y_test, predictions, proba)\n",
    "printscores_xv(scores)\n",
    "\n",
    "print(\"Training time: \" + str(training_time) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d0cf3-36bf-4604-ac07-5fc39fda4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# second round\n",
    "# Existing hyperparameters\n",
    "hyperparameters = {\n",
    "    'NN_TORCH': {},\n",
    "    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
    "    'CAT': {},\n",
    "    'XGB': {},\n",
    "    'FASTAI': {},\n",
    "    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini'}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr'}}],\n",
    "    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini'}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr'}}],\n",
    "    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
    "    # Add Logistic Regression\n",
    "    'LR': {}  # Using default hyperparameters for Logistic Regression\n",
    "}\n",
    "\n",
    "# Fit the predictor\n",
    "predictor2 = TabularPredictor(label='sirna_encoded', problem_type='multiclass').fit(\n",
    "    train_data=train_df,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f333b-0807-4e6f-854a-fb335532a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, log_loss\n",
    "\n",
    "y_true = test_df['sirna_encoded']\n",
    "y_pred = predictor2.predict(test_df.drop(columns=['sirna_encoded']))\n",
    "proba_pred = predictor2.predict_proba(test_df.drop(columns=['sirna_encoded']))\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "auc_roc = roc_auc_score(y_true, proba_pred, multi_class='ovr')\n",
    "logloss = log_loss(y_true, proba_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC-ROC: {auc_roc}\")\n",
    "print(f\"Log Loss: {logloss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbc1a6-961b-483b-9b4b-6e7a3c2f1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42, max_iter=10)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf6327-6c58-48b5-b940-0fe200e687de",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42, max_iter=1000, tol=1e-5, solver='saga', penalty='l1', class_weight='balanced', C=1.0)\n",
    "\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18183b09-9565-4c85-b27c-d7e55112669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_data = merged_df[feature_cols + ['sirna']]\n",
    "train_df, test_df = train_test_split(all_data, test_size=0.3, random_state=42)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "train_df['sirna_encoded'] = encoder.fit_transform(train_df['sirna'])\n",
    "test_df['sirna_encoded'] = encoder.transform(test_df['sirna'])\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['sirna_encoded']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['sirna_encoded']  \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14605eed-f9df-4ee2-8a83-8af1c4c4657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'logreg__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'logreg__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validated score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_score:.4f}\")\n",
    "\n",
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "training_time = time() - start_time\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "y_proba = best_model.predict_proba(X_test)\n",
    "test_auc_roc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc_roc:.4f}\")\n",
    "print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79bc677-9c22-45a4-89fa-38203cb59092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d1903-c544-4526-84f2-6ac85152b38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
